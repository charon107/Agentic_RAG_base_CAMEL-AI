"""
æ•°æ®åŠ è½½å™¨ - å°† OCR æ•°æ®å¯¼å…¥å‘é‡æ•°æ®åº“
æ”¯æŒæ–¹æ¡ˆBï¼šæŒ‰å¥/å›ºå®šé•¿åº¦åˆ†å—ï¼Œå¹¶å†™å…¥åˆ†å—å…ƒæ•°æ®åˆ°payload
"""

import json
import os
from typing import List, Dict
from qdrant import QdrantDB
import re


class DataLoader:
    """OCRæ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, qdrant_db: QdrantDB):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        å‚æ•°:
            qdrant_db: QdrantDBæ•°æ®åº“å®ä¾‹
        """
        self.qdrant_db = qdrant_db
    
    def load_ocr_data(
        self,
        json_file_path: str,
        enable_dedup: bool = True,
        chunking: str = "sentence",  
        chunk_size: int = 300,
        overlap: int = 50,
    ) -> int:
        """
        åŠ è½½OCRæ•°æ®åˆ°å‘é‡æ•°æ®åº“
        
        å‚æ•°:
            json_file_path: OCRæ•°æ®JSONæ–‡ä»¶è·¯å¾„
            enable_dedup: æ˜¯å¦å¯ç”¨å»é‡
            chunking: åˆ†å—æ–¹å¼
            chunk_size: ç›®æ ‡åˆ†å—é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
            overlap: åˆ†å—é‡å ï¼ˆå­—ç¬¦æ•°ï¼‰
            
        è¿”å›:
            æˆåŠŸå¯¼å…¥çš„æ–‡æœ¬æ•°é‡
        """
        if not os.path.exists(json_file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
        
        # è¯»å–JSONæ•°æ®
        with open(json_file_path, 'r', encoding='utf-8') as f:
            ocr_data = json.load(f)
        
        success_count = 0
        duplicate_count = 0
        seen_texts = set() if enable_dedup else None
        
        print(f"å¼€å§‹å¯¼å…¥OCRæ•°æ®ï¼Œå…± {len(ocr_data)} æ¡è®°å½•...")
        print(f"åˆ†å—é…ç½®: chunking={chunking}, chunk_size={chunk_size}, overlap={overlap}, enable_dedup={enable_dedup}")
        
        for i, item in enumerate(ocr_data):
            try:
                # æå–æ–‡æœ¬å†…å®¹
                text = item.get('text', '').strip()
                
                # è·³è¿‡ç©ºæ–‡æœ¬
                if not text:
                    continue
                
                # åˆ†å—ï¼šä»…æ”¯æŒæŒ‰å¥åˆ‡åˆ†ï¼›å…¶ä»–æƒ…å†µä¸åˆ†å—
                chunks: List[str]
                if chunking == "sentence":
                    chunks = self._split_sentences(text)
                else:
                    chunks = [text]

                # é¡µé¢ä¸ç±»å‹ä¿¡æ¯
                page_idx = item.get('page_idx', 0)
                text_type = item.get('type', 'text')
                
                # æ·»åŠ åˆ†å—è°ƒè¯•æ—¥å¿—
                print(f"  ğŸ“„ ç¬¬{i}æ¡è®°å½•: page_idx={page_idx}, type={text_type}, åŸæ–‡é•¿åº¦={len(text)}, åˆ†å—æ•°={len(chunks)}")

                # éå†æ¯ä¸ªåˆ†å—ï¼Œåšå»é‡å¹¶å†™å…¥
                total_chunks = len(chunks)
                for ci, chunk in enumerate(chunks, start=1):
                    chunk_clean = chunk.strip()
                    if not chunk_clean:
                        continue

                    if enable_dedup:
                        h = hash(chunk_clean)
                        if h in seen_texts:
                            duplicate_count += 1
                            continue
                        seen_texts.add(h)

                    # æ¥æºæ ‡è¯†ï¼šå¸¦åˆ†å—ä¿¡æ¯
                    source_file = f"OCR_page_{page_idx}_{text_type}"
                    if total_chunks > 1:
                        source_file += f"_ch{ci}of{total_chunks}"

                    # åˆ†å—å…ƒæ•°æ®
                    payload_extra = {
                        "page_idx": page_idx,
                        "type": text_type,
                        "chunk_index": ci,
                        "chunk_count": total_chunks,
                        "chunking": chunking,
                        "chunk_size": chunk_size,
                        "overlap": overlap,
                    }

                    # ä¿å­˜åˆ°æ•°æ®åº“
                    print(f"    ğŸ’¾ å†™å…¥åˆ†å—{ci}/{total_chunks}: source_file='{source_file}', é•¿åº¦={len(chunk_clean)}")
                    print(f"       å…ƒæ•°æ®: {payload_extra}")
                    self.qdrant_db.save_text(chunk_clean, source_file, payload_extra=payload_extra)
                    success_count += 1
                    print(f"    âœ… æˆåŠŸå†™å…¥åˆ†å—{ci}")
                
                # æ˜¾ç¤ºè¿›åº¦
                if (i + 1) % 10 == 0:
                    print(f"å·²å¤„ç† {i + 1}/{len(ocr_data)} æ¡è®°å½•...")
                    
            except Exception as e:
                print(f"å¤„ç†ç¬¬ {i} æ¡è®°å½•æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"æ•°æ®å¯¼å…¥å®Œæˆï¼æˆåŠŸå¯¼å…¥ {success_count} æ¡æ–‡æœ¬è®°å½•")
        if enable_dedup and duplicate_count > 0:
            print(f"ğŸ” å»é‡ç»Ÿè®¡ï¼šè·³è¿‡ {duplicate_count} æ¡é‡å¤è®°å½•")
        return success_count
    
    def batch_save_texts(self, texts: List[str], source_prefix: str = "batch") -> int:
        """
        æ‰¹é‡ä¿å­˜æ–‡æœ¬åˆ—è¡¨
        
        å‚æ•°:
            texts: æ–‡æœ¬åˆ—è¡¨
            source_prefix: æºæ–‡ä»¶å‰ç¼€
            
        è¿”å›:
            æˆåŠŸä¿å­˜çš„æ•°é‡
        """
        success_count = 0
        
        for i, text in enumerate(texts):
            try:
                if text and text.strip():
                    source_file = f"{source_prefix}_{i}"
                    self.qdrant_db.save_text(text.strip(), source_file)
                    success_count += 1
            except Exception as e:
                print(f"ä¿å­˜ç¬¬ {i} æ¡æ–‡æœ¬æ—¶å‡ºé”™: {e}")
                continue
        
        return success_count

    def _split_sentences(self, text: str) -> List[str]:
        """æŒ‰ä¸­æ–‡ä¸å¸¸è§æ ‡ç‚¹æ–­å¥ï¼Œä¿ç•™æ ‡ç‚¹ã€‚"""
        if not text:
            return []
        parts = re.split(r'([ã€‚ï¼ï¼Ÿ!?ï¼›;ï¼š:])', text)
        sentences: List[str] = []
        for i in range(0, len(parts), 2):
            seg = parts[i].strip()
            if not seg:
                continue
            punct = parts[i + 1] if i + 1 < len(parts) else ''
            sentences.append((seg + punct).strip())
        return sentences

